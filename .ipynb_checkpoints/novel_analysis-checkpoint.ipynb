{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text:\n",
    "    def __init__(self, path, title=\"\"):\n",
    "        self.title = title\n",
    "        self.path = path\n",
    "        self.text = open(self.path, encoding='utf-8').read()\n",
    "        self.word_count = 0\n",
    "        self.sentence_count = 0\n",
    "        self.dialogue_word_count = 0\n",
    "        self.parag_count = 0\n",
    "        self.vocab_count = 0\n",
    "        self.generate_stats()\n",
    "        \n",
    "    def generate_stats(self):\n",
    "        chunks = self.partition_text()\n",
    "        unique_words = set()\n",
    "        for i in chunks:\n",
    "            doc = nlp(i, disable=['NER'])\n",
    "            words, diag_words = self.count_words(doc)\n",
    "            self.word_count += words\n",
    "            self.dialogue_word_count += diag_words\n",
    "            self.sentence_count += len(list(doc.sents))\n",
    "            self.parag_count += self.parag_counter(doc)\n",
    "            unique_words = unique_words.union(set([i.lemma_ for i in doc if not i.is_punct and i.text not in [\"\\n\", \"\\n\\n\"]]))\n",
    "            del doc\n",
    "        self.vocab_count = len(unique_words)\n",
    "    \n",
    "    # NEEDS OVERHAUL\n",
    "    def count_words(self, doc):\n",
    "        \"\"\"Takes a spacy doc and returns a 2-tuple of overall word count and dialogue word count.\"\"\"\n",
    "        words_with_quotations = [token.text for token in doc if \n",
    "                                 (not token.is_punct or token.text=='\"' or token.text==\"“\" \n",
    "                                  or token.text==\"”\") and token.text!=\"\\n\\n\" or token.text!=\"\\n\"]\n",
    "        word_count = 0\n",
    "        diag_count = 0\n",
    "        diag = False\n",
    "        thing = []\n",
    "        for i in words_with_quotations:\n",
    "            if i in ['\"', \"“\", \"”\"]:\n",
    "                diag = not diag\n",
    "                continue\n",
    "            if diag:\n",
    "                diag_count += 1\n",
    "                thing.append(i) #get rid of thing stuff\n",
    "            word_count += 1\n",
    "        print(thing[0:1000])\n",
    "        return (word_count, diag_count)\n",
    "    \n",
    "    def partition_text(self):\n",
    "        chunks = []\n",
    "        while len(self.text) > 60000:\n",
    "            split_at = 50000\n",
    "            while self.text[split_at] != \".\":\n",
    "                split_at += 1\n",
    "            chunks.append(self.text[0:split_at+1])\n",
    "            self.text = self.text[split_at+1:]\n",
    "        chunks.append(self.text)\n",
    "        return chunks\n",
    "    \n",
    "    def parag_counter(self, doc):\n",
    "        parag_count = 1\n",
    "        for i in [token.text for token in doc]:\n",
    "            if i == \"\\n\\n\" or i == \"\\n\":\n",
    "                parag_count += 1\n",
    "        return parag_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"The Speedrunner and the Kid\", \"The Order of the Bell\", \"Gravity's Rainbow\"]#, \"The Left Hand of Darkness\",\n",
    "         #\"The Shining\"]\n",
    "files = [\"C:/Users/awitt/Desktop/spd_rnr.txt\", \"C:/Users/awitt/Desktop/ordr_bll.txt\", \"C:/Users/awitt/Desktop/grv_rnb.txt\"]\n",
    "         #, \"C:/Users/awitt/Desktop/lft_hnd.txt\", \n",
    "         #\"C:/Users/awitt/Desktop/shng.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'did', 'n’t', 'really', 'believe', 'you', '’d', 'be', 'saved', '.', 'Come', ',', 'we', 'all', 'know', 'who', 'we', 'are', 'by', 'now', '.', 'No', 'one', 'was', 'ever', 'going', 'to', 'take', 'the', 'trouble', 'to', 'save', 'you', ',', 'old', 'fellow', '.', '.', '.', '.', 'Pirate', 'Good', 'morning', ',', 'Hhahh', ',', 'hhaahhh', '!', 'Incoming', 'mail', '.', 'Kill', 'myself', ',', 'The', 'Germans', 'will', 'do', 'it', 'for', 'you', '.', 'Guess', 'what', 'I', 'saw', 'from', 'the', 'roof', '.', 'That', 'V-2', 'on', 'the', 'way', '?', 'A4', ',', 'yes', '.', 'I', 'watched', 'it', 'out', 'the', 'window', '.', 'About', 'ten', 'minutes', 'ago', '.', 'Looked', 'queer', ',', 'did', 'n’t', 'it', '.', 'Have', 'n’t', 'heard', 'a', 'thing', 'since', ',', 'have', 'you', '.', 'It', 'must', 'have', 'fallen', 'short', '.', 'Out', 'to', 'sea', 'or', 'something', '.', 'Ten', 'minutes', '?', 'At', 'least', '.', 'Prentice', 'here', ',', 'did', 'you', 'have', 'anything', 'like', 'a', 'pip', 'from', 'Holland', 'a', 'moment', 'ago', '.', 'Aha', '.', 'Aha', '.', 'Yes', ',', 'we', 'saw', 'it', '.', 'They', 'lost', 'it', 'over', 'the', 'coast', '.', 'They', '’re', 'calling', 'it', 'premature', 'Brennschluss', '.', 'Cheer', 'up', ',', 'There', '’ll', 'be', 'more', '.', 'Saxophone', 'Your', 'employer', '.', 'It', '’s', 'not', 'fair', ',', 'I', 'have', 'n’t', 'even', 'done', 'me', 'morning', 'pushups', 'yet', '.', 'It', 'came', 'over', 'in', 'a', 'rather', 'delightful', 'way', ',', 'none', 'of', 'my', 'friends', 'are', 'that', 'clever', '.', 'All', 'my', 'mail', 'arrives', 'by', 'post', '.', 'Do', 'come', 'collect', 'it', ',', 'wo', 'n’t', 'you', ',', 'Prentice', '.', 'You', 'ca', 'n’t', 'run', 'a', 'war', 'on', 'gusts', 'of', 'emotion', '.', 'Eh', '?', 'Girl', 'Guides', 'start', 'pumping', 'water', '.', '.', '.', 'your', 'sound', 'will', 'be', 'the', 'sizzling', 'night', '.', '.', '.', 'eh', '?', 'Go', 'away', ',', 'or', 'I', 'will', 'call', 'a', 'policeman', '.', 'The', 'Changing', 'of', 'the', 'Guard', ',', 'Got', 'it', 'in', 'Hampstead', 'Heath', ',', 'just', 'sitting', 'breathing', ',', 'like', '.', '.', '.', 'going', 'in', ',', 'and', 'out', '.', '.', '.', 'Any', 'sort', 'of', 'sound', 'down', 'there', '?', 'Yes', ',', 'it', '’s', 'horrible', '.', '.', '.', 'like', 'a', 'stupendous', 'nose', 'sucking', 'in', 'snot', '.', '.', '.', 'wait', ',', 'now', 'it', '’s', '.', '.', '.', 'beginning', 'to', '.', '.', '.', 'oh', ',', 'no', '.', '.', '.', 'oh', ',', 'God', ',', 'I', 'ca', 'n’t', 'describe', 'it', ',', 'it', '’s', 'so', 'beast—', 'Tantivy', 'Johnny', 'Doughboy', 'Found', 'a', 'Rose', 'in', 'Ireland', 'He', 'does', 'have', 'some', 'rather', 'snappy', 'arrangements', ',', 'he', '’s', 'a', 'sort', 'of', 'American', 'George', 'Formby', ',', 'if', 'you', 'can', 'imagine', 'such', 'a', 'thing', ',', 'Darlene', 'he', 'tells', 'his', 'friend', 'Bloat', '.', 'thereupon', 'going', 'into', 'the', 'story', 'of', 'Lorraine', 'and', 'Judy', ',', 'Charles', 'the', 'homosexual', 'constable', 'and', 'the', 'piano', 'in', 'the', 'pantechnicon', ',', 'or', 'the', 'bizarre', 'masquerade', 'involving', 'Gloria', 'and', 'her', 'nubile', 'mother', ',', 'a', 'quid', 'wager', 'on', 'the', 'Blackpool', '-', 'Preston', 'North', 'End', 'game', ',', 'a', 'naughty', 'version', 'of', 'and', 'a', 'providential', 'fog', '.', 'But', 'none', 'of', 'these', 'yarns', ',', 'for', 'the', 'purposes', 'of', 'those', 'Bloat', 'reports', 'to', ',', 'are', 'really', 'very', 'illuminating', '.', '.', '.', '.', '\\n\\n\\t\\t\\t\\t ', 'Well', '.', 'He', '’s', 'done', 'now', '.', 'Bag', 'zipped', ',', 'lamp', 'off', 'and', 'moved', 'back', 'in', 'place', '.', 'Perhaps', 'there', '’s', 'time', 'to', 'catch', 'Tantivy', 'over', 'at', 'the', 'Snipe', 'and', 'Shaft', ',', 'time', 'for', 'a', 'comradely', 'pint', '.', 'He', 'moves', 'back', 'down', 'the', 'beaverboard', 'maze', ',', 'in', 'the', 'weak', 'yellow', 'light', ',', 'against', 'a', 'tide', 'of', 'incoming', 'girls', 'in', 'galoshes', ',', 'aloof', 'Bloat', 'unsmiling', ',', 'no', 'time', 'for', 'slap', '-', 'and', '-', 'tickle', 'here', 'you', 'see', ',', 'he', 'still', 'has', 'his', 'day', '’s', 'delivery', 'to', 'make', '.', '.', '.', '.', '\\n\\n\\t\\t\\t\\t ', '•', '•', '•', '•', '•', '•', '•', '\\n\\n\\t\\t\\t\\t ', 'Wind', 'has', 'shifted', 'around', 'to', 'the', 'southwest', ',', 'and', 'the', 'barometer', '’s', 'falling', '.', 'The', 'early', 'afternoon', 'is', 'already', 'dark', 'as', 'evening', ',', 'under', 'the', 'massing', 'rainclouds', '.', 'Tyrone', 'Slothrop', 'is', 'gon', 'na', 'be', 'caught', 'out', 'in', 'it', ',', 'too', '.', 'Today', 'it', '’s', 'been', 'a', 'long', ',', 'idiot', 'chase', 'out', 'to', 'zero', 'longitude', ',', 'with', 'the', 'usual', 'nothing', 'to', 'show', '.', 'This', 'one', 'was', 'supposed', 'to', 'be', 'another', 'premature', 'airburst', ',', 'the', 'lumps', 'of', 'burning', 'rocket', 'showering', 'down', 'for', 'miles', 'around', ',', 'most', 'of', 'it', 'into', 'the', 'river', ',', 'only', 'one', 'piece', 'in', 'any', 'kind', 'of', 'shape', 'and', 'that', 'well', 'surrounded', ',', 'by', 'the', 'time', 'Slothrop', 'arrived', ',', 'with', 'the', 'tightest', 'security', 'he', '’s', 'seen', 'yet', ',', 'and', 'the', 'least', 'friendly', '.', 'Soft', ',', 'faded', 'berets', 'against', 'the', 'slate', 'clouds', ',', 'Mark', 'III', 'Stens', 'set', 'on', 'automatic', ',', 'mustaches', 'mouthwide', 'covering', 'enormous', 'upper', 'lips', ',', 'humorless', '—', 'no', 'chance', 'for', 'any', 'American', 'lieutenant', 'to', 'get', 'a', 'look', ',', 'not', 'today', '.', '\\n\\n\\t\\t\\t\\t ', 'ACHTUNG', ',', 'anyhow', ',', 'is', 'the', 'poor', 'relative', 'of', 'Allied', 'intelligence', '.', 'At', 'least', 'this', 'time', 'Slothrop', '’s', 'not', 'alone', ',', 'he', '’s', 'had', 'the', 'cold', 'comfort', 'of', 'seeing', 'his', 'opposite', 'number', 'from', 'T.I.', ',', 'and', 'shortly', 'after', 'that', 'even', 'the', 'man', '’s', 'section', 'chief', ',', 'come', 'fussing', 'onto', 'the', 'scene', 'in', 'a', '’37', 'Wolseley', 'Wasp', ',', 'both', 'turned', 'back', 'too', '.', 'Ha', '!', 'Neither', 'of', 'them', 'returning', 'Slothrop', '’s', 'amiable', 'nod', '.', 'Tough', 'shit', ',', 'fellas', '.', 'But', 'shrewd', 'Tyrone', 'hangs', 'around', ',', 'distributing', 'Lucky', 'Strikes', ',', 'long', 'enough', 'to', 'find', 'at', 'least', 'what', '’s', 'up', 'with', 'this', 'Unlucky', 'Strike', ',', 'here', '.', '\\n\\n\\t\\t\\t\\t ', 'What', 'it', 'is', 'is', 'a', 'graphite', 'cylinder', ',', 'about', 'six', 'inches', 'long', 'and', 'two', 'in', 'diameter', ',', 'all', 'but', 'a', 'few', 'flakes', 'of', 'its', 'Army', '-', 'green', 'paint', 'charred', 'away', '.', 'Only', 'piece', 'that', 'survived', 'the', 'burst', '.', 'Evidently', 'it', 'was', 'meant', 'to', '.', 'There', 'seem', 'to', 'be', 'papers', 'stashed', 'inside', '.', 'Sergeant', '-', 'major', 'burned', 'his', 'hand', 'picking', 'it', 'up', 'and', 'was', 'heard', 'to', 'holler', 'Oh', 'fuck', ',', 'causing', 'laughter', 'among', 'the', 'lower', 'paygrades', '.', 'Everybody', 'was', 'waiting', 'around', 'for', 'a', 'Captain', 'Prentice', 'from', 'S.O.E.', '(', 'those', 'prickly', 'bastards', 'take', 'their', 'time', 'about', 'everything', ')', ',', 'who', 'does', 'presently', 'show', 'up', '.', 'Slothrop', 'gets', 'a', 'glimpse', '—', 'windburned', 'face', ',', 'big', 'mean', 'mother', '.', 'Prentice', 'takes', 'the', 'cylinder', ',', 'drives', 'away', ',', 'and', 'that', '’s', 'that', '.', '\\n\\n\\t\\t\\t\\t ', 'In', 'which', 'case', ',', 'Slothrop', 'reckons', ',', 'ACHTUNG', 'can', ',', 'a', 'bit', 'wearily', ',', 'submit', 'its', 'fifty', '-', 'millionth', 'interbranch', 'request', 'to', 'that', 'S.O.E.', ',', 'asking', 'for', 'some', 'report']\n",
      "['I', 'know', 'there', 'is', 'wilde', 'love', 'and', 'joy', 'enough', 'in', 'the', 'world', ',', 'as', 'there', 'are', 'wilde', 'Thyme', ',', 'and', 'other', 'herbes', ';', 'but', 'we', 'would', 'have', 'garden', 'love', ',', 'and', 'garden', 'joy', ',', 'of', 'Gods', 'owne', 'planting', '.', 'What', 'happened', '?', 'Your', 'two', 'Wrens', '.', '.', '.', 'when', 'they', 'saw', 'you', '.', '.', '.', 'Slothrop—', 'I', 'do', 'n’t', 'know', '.', 'Jesus', '.', 'You', 'ca', 'n’t', 'hear', 'them', 'when', 'they', 'come', 'in', '.', 'they', '.', 'Of', 'course', 'you', 'ca', 'n’t', ',', 'they', 'go', 'faster', 'than', 'sound', '.', 'Yes', 'but', '—', 'that', '’s', 'not', 'it', ',', '\\n\\n\\t\\t\\t\\t ', '\\n\\n\\t\\t\\t\\t ', '\\n\\n\\t\\t\\t\\t ', '\\n\\n\\t\\t\\t\\t ', 'teeth', 'chattering', ',', '\\n\\n\\t\\t\\t\\t ', 'Tantivy', ',', 'leaning', 'anxiously', 'through', 'the', 'smell', 'of', 'hops', 'and', 'the', 'brown', 'gloom', ',', 'more', 'worried', 'now', 'about', 'Slothrop', '’s', 'shaking', 'than', 'any', 'specter', 'of', 'his', 'own', ',', 'has', 'nothing', 'but', 'established', 'channels', 'he', 'happens', 'to', 'know', 'of', 'to', 'try', 'and', 'conjure', 'it', 'away', '.', '\\n\\n\\t\\t\\t\\t ', '\\n\\n\\t\\t\\t\\t ', '\\n\\n\\t\\t\\t\\t ', 'Which', 'is', 'how', 'Slothrop', 'got', 'into', 'investigating', 'V', '-', 'bomb', 'Aftermaths', '.', 'Each', 'morning', '—', 'at', 'first', '—', 'someone', 'in', 'Civil', 'Defence', 'routed', 'ACHTUNG', 'a', 'list', 'of', 'yesterday', '’s', 'hits', '.', 'It', 'would', 'come', 'round', 'to', 'Slothrop', 'last', ',', 'he', '’d', 'detach', 'its', 'pencil', '-', 'smeared', 'buck', 'slip', ',', 'go', 'draw', 'the', 'same', 'aging', 'Humber', 'from', 'the', 'motor', 'pool', ',', 'and', 'make', 'his', 'rounds', ',', 'a', 'Saint', 'George', 'after', 'the', 'fact', ',', 'going', 'out', 'to', 'poke', 'about', 'for', 'droppings', 'of', 'the', 'Beast', ',', 'fragments', 'of', 'German', 'hardware', 'that', 'would', 'n’t', 'exist', ',', 'writing', 'empty', 'summaries', 'into', 'his', 'notebooks', '—', 'work', '-', 'therapy', '.', 'As', 'inputs', 'to', 'ACHTUNG', 'got', 'faster', ',', 'often', 'he', '’d', 'show', 'up', 'in', 'time', 'to', 'help', 'the', 'search', 'crews', '—', 'following', 'restless', '-', 'muscled', 'RAF', 'dogs', 'into', 'the', 'plaster', 'smell', ',', 'the', 'gas', 'leaking', ',', 'the', 'leaning', 'long', 'splinters', 'and', 'sagging', 'mesh', ',', 'the', 'prone', 'and', 'noseless', 'caryatids', ',', 'rust', 'already', 'at', 'nails', 'and', 'naked', 'threadsurfaces', ',', 'the', 'powdery', 'wipe', 'of', 'Nothing', '’s', 'hand', 'across', 'wallpaper', 'awhisper', 'with', 'peacocks', 'spreading', 'their', 'fans', 'down', 'deep', 'lawns', 'to', 'Georgian', 'houses', 'long', 'ago', ',', 'to', 'safe', 'groves', 'of', 'holm', 'oak', '.', '.', '.', 'among', 'the', 'calls', 'for', 'silence', 'following', 'to', 'where', 'some', 'exposed', 'hand', 'or', 'brightness', 'of', 'skin', 'waited', 'them', ',', 'survivor', 'or', 'casualty', '.', 'When', 'he', 'could', 'n’t', 'help', 'he', 'stayed', 'clear', ',', 'praying', ',', 'at', 'first', ',', 'conventionally', 'to', 'God', ',', 'first', 'time', 'since', 'the', 'other', 'Blitz', ',', 'for', 'life', 'to', 'win', 'out', '.', 'But', 'too', 'many', 'were', 'dying', ',', 'and', 'presently', ',', 'seeing', 'no', 'point', ',', 'he', 'stopped', '.', '\\n\\n\\t\\t\\t\\t ', 'Yesterday', 'happened', 'to', 'be', 'a', 'good', 'day', '.', 'They', 'found', 'a', 'child', ',', 'alive', ',', 'a', 'little', 'girl', ',', 'half', '-', 'suffocated', 'under', 'a', 'Morrison', 'shelter', '.', 'Waiting', 'for', 'the', 'stretcher', ',', 'Slothrop', 'held', 'her', 'small', 'hand', ',', 'gone', 'purple', 'with', 'the', 'cold', '.', 'Dogs', 'barked', 'in', 'the', 'street', '.', 'When', 'she', 'opened', 'her', 'eyes', 'and', 'saw', 'him', 'her', 'first', 'words', 'were', ',', 'Trapped', 'there', 'for', 'two', 'days', ',', 'gum', '-', 'less', '—', 'all', 'he', 'had', 'for', 'her', 'was', 'a', 'Thayer', '’s', 'Slippery', 'Elm', '.', 'He', 'felt', 'like', 'an', 'idiot', '.', 'Before', 'they', 'took', 'her', 'off', 'she', 'brought', 'his', 'hand', 'over', 'to', 'kiss', 'anyway', ',', 'her', 'mouth', 'and', 'cheek', 'in', 'the', 'flare', 'lamps', 'cold', 'as', 'frost', ',', 'the', 'city', 'around', 'them', 'at', 'once', 'a', 'big', 'desolate', 'icebox', ',', 'stale', '-', 'smelling', 'and', 'no', 'surprises', 'inside', 'ever', 'again', '.', 'At', 'which', 'point', 'she', 'smiled', ',', 'very', 'faintly', ',', 'and', 'he', 'knew', 'that', '’s', 'what', 'he', '’d', 'been', 'waiting', 'for', ',', 'wow', ',', 'a', 'Shirley', 'Temple', 'smile', ',', 'as', 'if', 'this', 'exactly', 'canceled', 'all', 'they', '’d', 'found', 'her', 'down', 'in', 'the', 'middle', 'of', '.', 'What', 'a', 'damn', 'fool', 'thing', '.', 'He', 'hangs', 'at', 'the', 'bottom', 'of', 'his', 'blood', '’s', 'avalanche', ',', '300', 'years', 'of', 'western', 'swamp', '-', 'Yankees', ',', 'and', 'ca', 'n’t', 'manage', 'but', 'some', 'nervous', 'truce', 'with', 'their', 'Providence', '.', 'A', 'détente', '.', 'Ruins', 'he', 'goes', 'daily', 'to', 'look', 'in', 'are', 'each', 'a', 'sermon', 'on', 'vanity', '.', 'That', 'he', 'finds', ',', 'as', 'weeks', 'wear', 'on', ',', 'no', 'least', 'fragment', 'of', 'any', 'rocket', ',', 'preaches', 'how', 'indivisible', 'is', 'the', 'act', 'of', 'death', '.', '.', '.', 'Slothrop', '’s', 'Progress', ':', 'London', 'the', 'secular', 'city', 'instructs', 'him', ':', 'turn', 'any', 'corner', 'and', 'he', 'can', 'find', 'himself', 'inside', 'a', 'parable', '.', '\\n\\n\\t\\t\\t\\t ', 'He', 'has', 'become', 'obsessed', 'with', 'the', 'idea', 'of', 'a', 'rocket', 'with', 'his', 'name', 'written', 'on', 'it', '—', 'if', 'they', '’re', 'really', 'set', 'on', 'getting', 'him', '(', 'embracing', 'possibilities', 'far', 'far', 'beyond', 'Nazi', 'Germany', ')', 'that', '’s', 'the', 'surest', 'way', ',', 'does', 'n’t', 'cost', 'them', 'a', 'thing', 'to', 'paint', 'his', 'name', 'on', 'every', 'one', ',', 'right', '?', '\\n\\n\\t\\t\\t\\t ', 'Tantivy', 'watching', 'him', 'funny', ',', '\\n\\n\\t\\t\\t\\t ', 'lighting', 'a', 'cigarette', ',', 'shaking', 'his', 'forelock', 'through', 'the', 'smoke', ',', '\\n\\n\\t\\t\\t\\t ', 'It', '’s', 'nothing', 'he', 'can', 'see', 'or', 'lay', 'hands', 'on', '—', 'sudden', 'gases', ',', 'a', 'violence', 'upon', 'the', 'air', 'and', 'no', 'trace', 'afterward', '.', '.', '.', 'a', 'Word', ',', 'spoken', 'with', 'no', 'warning', 'into', 'your', 'ear', ',', 'and', 'then', 'silence', 'forever', '.', 'Beyond', 'its', 'invisibility', ',', 'beyond', 'hammerfall', 'and', 'doomcrack', ',', 'here', 'is', 'its', 'real', 'horror', ',', 'mocking', ',', 'promising', 'him', 'death', 'with', 'German', 'and', 'precise', 'confidence', ',', 'laughing', 'down', 'all', 'of', 'Tantivy', '’s', 'quiet', 'decencies', '.', '.', '.', 'no', ',', 'no', 'bullet', 'with', 'fins', ',', 'Ace', '.', '.', '.', 'not', 'the', 'Word', ',', 'the', 'one', 'Word', 'that', 'rips', 'apart', 'the', 'day', '.', '.', '.', '.', '\\n\\n\\t\\t\\t\\t ', 'It', 'was', 'Friday', 'evening', ',', 'last', 'September', ',', 'just', 'off', 'work', ',', 'heading', 'for', 'the', 'Bond', 'Street', 'Underground', 'station', ',', 'his', 'mind', 'on', 'the', 'weekend', 'ahead', 'and', 'his', 'two', 'Wrens', ',', 'that', 'Norma', 'and', 'that', 'Marjorie', ',', 'whom', 'he', 'must', 'each', 'keep', 'from', 'learning', 'about', 'the', 'other', ',', 'just', 'as', 'he', 'was', 'reaching', 'to', 'pick', 'his', 'nose', ',', 'suddenly', 'in', 'the', 'sky', ',', 'miles', 'behind', 'his', 'back', 'and', 'up', 'the', 'river', 'mementomori', 'a', 'sharp', 'crack', 'and', 'a', 'heavy', 'explosion', ',', 'rolling', 'right', 'behind', ',', 'almost', 'like', 'a', 'clap', 'of', 'thunder', '.', 'But', 'not', 'quite', '.', 'Seconds', 'later', ',', 'this', 'time', 'from', 'in', 'front', 'of', 'him', ',', 'it', 'happened', 'again', ':', 'loud', 'and', 'clear', ',', 'all', 'over', 'the']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-0d2b9d06b2b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"grav\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"C:/Users/awitt/Desktop/grv_rnb.txt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mBook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-0d2b9d06b2b0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"grav\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"C:/Users/awitt/Desktop/grv_rnb.txt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mBook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-92b34293a3ac>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, path)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparag_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-92b34293a3ac>\u001b[0m in \u001b[0;36mgenerate_stats\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0munique_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NER'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiag_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__call__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datasci\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(seqs_in, drop)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad),\n\u001b[0m\u001b[0;32m    280\u001b[0m                                          drop=drop)\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.Ops.flatten\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "title = [\"grav\"]\n",
    "files = [\"C:/Users/awitt/Desktop/grv_rnb.txt\"]\n",
    "books = [Book(i, j) for (i, j) in zip(titles, files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Speedrunner and the Kid\n",
      "66850\n",
      "6909\n",
      "0.48463724756918475\n",
      "4497\n",
      "The Order of the Bell\n",
      "142265\n",
      "18096\n",
      "0.44791058939303413\n",
      "6968\n",
      "Gravity's Rainbow\n",
      "414955\n",
      "28840\n",
      "0.47942547987131134\n",
      "20844\n"
     ]
    }
   ],
   "source": [
    "for i in books:\n",
    "    print(i.title)\n",
    "    print(i.word_count)\n",
    "    print(i.sentence_count)\n",
    "    print(i.dialogue_word_count / i.word_count)\n",
    "    print(i.vocab_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spd has 6909 sentences\n",
      "ordr has 18098 sentences\n"
     ]
    }
   ],
   "source": [
    "# Number of sentences in each book\n",
    "spd_num_sents = len(list(spd_doc.sents))\n",
    "ordr_num_sents = len(list(ordr_doc.sents))\n",
    "print(\"spd has \" + str(spd_num_sents) + \" sentences\")\n",
    "print(\"ordr has \" + str(ordr_num_sents) + \" sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spd has 58026 words\n",
      "ordr has 123002 words\n",
      "spd has 193 pages\n",
      "ordr has 410 pages\n"
     ]
    }
   ],
   "source": [
    "# Number of words in each book\n",
    "spd_words = [token for token in spd_doc if not token.is_punct]\n",
    "ordr_words = [token for token in ordr_doc if not token.is_punct]\n",
    "spd_num_words = len(spd_words)\n",
    "ordr_num_words = len(ordr_words)\n",
    "spd_num_pages = len(spd_words)//300\n",
    "ordr_num_pages = len(ordr_words)//300\n",
    "print(\"spd has \" + str(spd_num_words) + \" words\")\n",
    "print(\"ordr has \" + str(ordr_num_words) + \" words\")\n",
    "print(\"spd has \" + str(spd_num_pages) + \" pages\")\n",
    "print(\"ordr has \" + str(ordr_num_pages) + \" pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spd has 8.4 words per sentence\n",
      "ordr has 6.8 words per sentence\n"
     ]
    }
   ],
   "source": [
    "# Number of words per sentence\n",
    "spd_words_per_sentence = round(len(spd_words)/len(list(spd_doc.sents)), 2)\n",
    "ordr_words_per_sentence = round(len(ordr_words)/len(list(ordr_doc.sents)), 2)\n",
    "print(\"spd has \" + str(spd_words_per_sentence) + \" words per sentence\")\n",
    "print(\"ordr has \" + str(ordr_words_per_sentence) + \" words per sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_words = [token.text for token in spd_doc if not token.is_punct]\n",
    "ordr_words = [token.text for token in ordr_doc if not token.is_punct]\n",
    "spd_word_freq = Counter(spd_words)\n",
    "ordr_word_freq = Counter(ordr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "shit_stops = ['cee', '\\n\\n', '\\n', \"'s\", 'n’t', '’s', \"'d\", \"'m\", \"'re\", '’d', '’re', '’m', \"'ve\", \"'ll\", '’ll', '’ve']\n",
    "spd_stops = ['Macuahuitl', 'worldtree', 'felixthebeast', \"n't\", \"Nikolai\", \"Gard\", \"re\", \"TlalocsKid\", \"ll\", \"Monica\", \"said\", 've', 'EvilMollusc', 'WorldTree54', 'Reidar']\n",
    "ordr_stops = [\"Marto\", \"Claire\", \"Wendell\", \"Deborah\", \"Ben\", \"Alex\", \"Finch\", \"John\", \"re\", \"ve\", \"ll\", \"Mephisto\", \"said\", \"Khiver\", \"Adrienne\", \"Khemenehadra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_ed_words = [token.text.lower() for token in spd_doc if not token.is_punct and not token.is_stop]\n",
    "spd_ed_words = [i for i in spd_ed_words if i not in [j.lower() for j in spd_stops] and i not in STOPWORDS and i not in shit_stops]\n",
    "ordr_ed_words = [token.text.lower() for token in ordr_doc if not token.is_punct and not token.is_stop]\n",
    "ordr_ed_words = [i for i in ordr_ed_words if i not in [j.lower() for j in ordr_stops] and i not in STOPWORDS and i not in shit_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_freq_df = pd.DataFrame(list(Counter(spd_ed_words).most_common(25)), columns=[\"Word\", \"Uses\"])\n",
    "spd_freq_df[\"Uses per Page\"] = round(spd_freq_df[\"Uses\"]/spd_num_pages, 2)\n",
    "spd_freq_df.index +=1\n",
    "\n",
    "ordr_freq_df = pd.DataFrame(list(Counter(ordr_ed_words).most_common(25)), columns=[\"Word\", \"Uses\"])\n",
    "ordr_freq_df[\"Uses per Page\"] = round(ordr_freq_df[\"Uses\"]/ordr_num_pages, 2)\n",
    "ordr_freq_df.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output {\n",
       "    flex-direction: row;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "CSS = \"\"\"\n",
    ".output {\n",
    "    flex-direction: row;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "HTML('<style>{}</style>'.format(CSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Uses</th>\n",
       "      <th>Uses per Page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>time</td>\n",
       "      <td>171</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kid</td>\n",
       "      <td>150</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right</td>\n",
       "      <td>141</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>father</td>\n",
       "      <td>137</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>way</td>\n",
       "      <td>134</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>going</td>\n",
       "      <td>126</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>want</td>\n",
       "      <td>106</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>long</td>\n",
       "      <td>101</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>let</td>\n",
       "      <td>94</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sure</td>\n",
       "      <td>93</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>little</td>\n",
       "      <td>92</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>know</td>\n",
       "      <td>90</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>think</td>\n",
       "      <td>84</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>old</td>\n",
       "      <td>81</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>got</td>\n",
       "      <td>80</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>life</td>\n",
       "      <td>76</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fucking</td>\n",
       "      <td>73</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>face</td>\n",
       "      <td>73</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>eyes</td>\n",
       "      <td>72</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>maybe</td>\n",
       "      <td>70</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>game</td>\n",
       "      <td>70</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>good</td>\n",
       "      <td>67</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>day</td>\n",
       "      <td>67</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>need</td>\n",
       "      <td>66</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>come</td>\n",
       "      <td>62</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Uses  Uses per Page\n",
       "1      time   171           0.89\n",
       "2       kid   150           0.78\n",
       "3     right   141           0.73\n",
       "4    father   137           0.71\n",
       "5       way   134           0.69\n",
       "6     going   126           0.65\n",
       "7      want   106           0.55\n",
       "8      long   101           0.52\n",
       "9       let    94           0.49\n",
       "10     sure    93           0.48\n",
       "11   little    92           0.48\n",
       "12     know    90           0.47\n",
       "13    think    84           0.44\n",
       "14      old    81           0.42\n",
       "15      got    80           0.41\n",
       "16     life    76           0.39\n",
       "17  fucking    73           0.38\n",
       "18     face    73           0.38\n",
       "19     eyes    72           0.37\n",
       "20    maybe    70           0.36\n",
       "21     game    70           0.36\n",
       "22     good    67           0.35\n",
       "23      day    67           0.35\n",
       "24     need    66           0.34\n",
       "25     come    62           0.32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Uses</th>\n",
       "      <th>Uses per Page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>told</td>\n",
       "      <td>238</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eyes</td>\n",
       "      <td>231</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time</td>\n",
       "      <td>225</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>room</td>\n",
       "      <td>221</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>turned</td>\n",
       "      <td>209</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hand</td>\n",
       "      <td>200</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>going</td>\n",
       "      <td>196</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>looked</td>\n",
       "      <td>184</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>door</td>\n",
       "      <td>184</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>right</td>\n",
       "      <td>174</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>moment</td>\n",
       "      <td>172</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>head</td>\n",
       "      <td>169</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>know</td>\n",
       "      <td>167</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>asked</td>\n",
       "      <td>163</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>later</td>\n",
       "      <td>161</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>way</td>\n",
       "      <td>150</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>body</td>\n",
       "      <td>145</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>went</td>\n",
       "      <td>145</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>took</td>\n",
       "      <td>143</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>got</td>\n",
       "      <td>143</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>need</td>\n",
       "      <td>137</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>let</td>\n",
       "      <td>132</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nodded</td>\n",
       "      <td>132</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stood</td>\n",
       "      <td>127</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>think</td>\n",
       "      <td>123</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  Uses  Uses per Page\n",
       "1     told   238           0.58\n",
       "2     eyes   231           0.56\n",
       "3     time   225           0.55\n",
       "4     room   221           0.54\n",
       "5   turned   209           0.51\n",
       "6     hand   200           0.49\n",
       "7    going   196           0.48\n",
       "8   looked   184           0.45\n",
       "9     door   184           0.45\n",
       "10   right   174           0.42\n",
       "11  moment   172           0.42\n",
       "12    head   169           0.41\n",
       "13    know   167           0.41\n",
       "14   asked   163           0.40\n",
       "15   later   161           0.39\n",
       "16     way   150           0.37\n",
       "17    body   145           0.35\n",
       "18    went   145           0.35\n",
       "19    took   143           0.35\n",
       "20     got   143           0.35\n",
       "21    need   137           0.33\n",
       "22     let   132           0.32\n",
       "23  nodded   132           0.32\n",
       "24   stood   127           0.31\n",
       "25   think   123           0.30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spd_freq_df)\n",
    "display(ordr_freq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x2ca8053e448>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORDCLOUD STUFF\n",
    "\n",
    "'''\n",
    "# spd generic wordcloud\n",
    "spd_stops = ['Macuahuitl', 'worldtree', 'felixthebeast', \"n't\", \"Nikolai\", \"Gard\", \"re\", \"TlalocsKid\", \"ll\", \"Monica\", \"said\", 've', 'EvilMollusc', 'WorldTree54', 'Reidar']\n",
    "spd_ed_cloud = WordCloud(max_words=60, stopwords=set(spd_stops)|STOPWORDS, width=800, height=400).generate(\" \".join(spd_ed_words))\n",
    "plt.imshow(spd_ed_cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# ordr generic wordcloud\n",
    "ordr_stops = [\"Marto\", \"Claire\", \"Wendell\", \"Deborah\", \"Ben\", \"Alex\", \"Finch\", \"John\", \"re\", \"ve\", \"ll\", \"Mephisto\", \"said\", \"Khiver\", \"Adrienne\", \"Khemenehadra\"]\n",
    "ordr_ed_cloud = WordCloud(max_words=60, stopwords=set(ordr_stops)|STOPWORDS, width=800, height=400).generate(\" \".join(ordr_ed_words))\n",
    "plt.imshow(ordr_ed_cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "spd_ed_cloud.to_file(\"C:/Users/awitt/Desktop/spd_cloud.png\")\n",
    "# https://imgur.com/a/l0o8YIs\n",
    "ordr_ed_cloud.to_file(\"C:/Users/awitt/Desktop/ordr_cloud.png\")\n",
    "# https://imgur.com/a/0gfbJc1\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
